{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = r\"C:\\Users\\DELL\\Desktop\\news\"  #Directory of data\n",
    "\n",
    "folders=sorted(os.listdir(os.path.join(DATA_DIR))) # os.listdir gives a list of all files in this path\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "data={}   # data is a dictionary of the form { folder1 : [doc1,doc2,....,doc1000] , folder2 : [doc1,doc2,doc3,....] }\n",
    "for folder in folders:\n",
    "    data[folder]=[]\n",
    "    for file in os.listdir(os.path.join(DATA_DIR,folder)):\n",
    "        with open(os.path.join(DATA_DIR,folder,file),encoding='latin-1') as opened_file:\n",
    "            data[folder].append(opened_file.read())\n",
    "            \n",
    "print(len(data[folders[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating vocabulary (feature set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Importing list of stop words from nltk\n",
    "from string import punctuation # Importing list of punctuations from string\n",
    "punctuations=list(punctuation)\n",
    "stopWords=stopwords.words('english')\n",
    "stopWords+=punctuations # Combined list of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also adding own list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common words throughout all docs play no part in classification ,so removing them\n",
    "stopWords+=['subject:','from:', 'date:', 'newsgroups:', 'message-id:', 'lines:', 'path:', 'organization:', \n",
    "            'would', 'writes:', 'references:', 'article', 'sender:', 'nntp-posting-host:', 'people', \n",
    "            'university', 'think', 'xref:', 'cantaloupe.srv.cs.cmu.edu', 'could', 'distribution:', 'first', \n",
    "            'anyone','world', 'really', 'since', 'right', 'believe', 'still', \n",
    "            \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390170"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I go by vocab -> each doc -> word in each doc.\n",
    "#Then check whether the word is stopword, add the word only if it is not a stopword.\n",
    "#Also after this check whether the word is already present in the vocab dictionary so as to maintain the freq count of the words to be added.\n",
    "\n",
    "vocab={}\n",
    "# Creating a dictionary of words and their frequency\n",
    "for i in range(len(data)): # For each key(newsgroup)\n",
    "    for doc in data[folders[i]]: # For each document corresponding to key(newsgroup)\n",
    "        for word in doc.split(): #for each word in that document and these words are obtained by splitting the document\n",
    "            if word.lower() not in stopWords and len(word.lower()) >= 5:\n",
    "                if word.lower() not in vocab:\n",
    "                    vocab[word.lower()]=1   #if the word is not present in the vocab then its initial freq =1\n",
    "                else:\n",
    "                    vocab[word.lower()]+=1  #if the word is present in vocab then increment its freq.\n",
    "                    \n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the dictionary based on the frequency of each 'possible' vocabulary word.\n",
    "#I am sorting the dictionary as I want top k words(means k words having max frequency in the sorted list) out of the dictionary.\n",
    "\n",
    "import operator\n",
    "sorted_vocab = sorted(vocab.items(), key = operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390170"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_vocab)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building final feature list from vocab by selecting top k frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing top 3000 vocab words as features\n",
    "feature_list = []\n",
    "for key in sorted_vocab:\n",
    "    feature_list.append(key[0])\n",
    "feature_list = feature_list[0: 3000]   # K = 3000 (number of words in vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming data into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[]   # list of newsgroups \n",
    "for i in range(len(data)):\n",
    "    for doc in data[folders[i]]:\n",
    "        Y.append(folders[i])\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[folders[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row : one doc  \n",
    "# Each column : one word from feature_list\n",
    "# Columns headers will be the names of features \n",
    "\n",
    "df = pd.DataFrame(columns = feature_list)\n",
    "\n",
    "for folder in folders:\n",
    "    # Insert each file as a new row \n",
    "    for file in os.listdir(os.path.join(DATA_DIR,folder)):\n",
    "        # Add a new row for every file\n",
    "        df.loc[len(df)] = np.zeros(len(feature_list))\n",
    "        with open(os.path.join(DATA_DIR, folder, file), encoding='latin-1') as opened_file:\n",
    "            for word in opened_file.read().split():\n",
    "                if word.lower() in feature_list:\n",
    "                    df[word.lower()][len(df)-1] += 1   #df[current_column][current_row]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values  #converting into numpy array for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting X and Y into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the inbuilt Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Multinomial Naive Bayes from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, y_train):\n",
    "    result = {}\n",
    "    result[\"total_data\"] = len(y_train)\n",
    "    class_labels = set(y_train)  #gives all unique values present  in y_train\n",
    "    for current_label in class_labels:\n",
    "        result[current_class] = {}\n",
    "        current_rows = (y_train == current_label)  #gives a True False array \n",
    "        x_train_current = X_train[current_rows]\n",
    "        y_train_current = y_train[current_rows]\n",
    "        total_words = 0\n",
    "        for i in range(len(feature_list)):\n",
    "            result[current_label][feature_list[i]] = X_train_current[:, i].sum()\n",
    "            total_words += X_train_current[:, i].sum()\n",
    "        result[current_label][\"total_count\"] = total_words\n",
    "    return result  #dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probability function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(x, dictionary, current_class):\n",
    "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    for i in range(len(feature_list)):\n",
    "        current_word_count = dictionary[current_class][feature_list[i]] + 1  # +1 for laplace correction\n",
    "        total_word_count = dictionary[current_class][\"total_count\"] + len(feature_list)\n",
    "        current_word_probability = np.log(current_word_count) - np.log(total_word_count)\n",
    "        for j in range(int(x[i])):  # if the frequency of word in test data point is zero then we wont consider it.\n",
    "            output += current_word_probability\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictSingleClass func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSingleClass(x,dictionary):\n",
    "    best_class = -1000\n",
    "    best_prob = -1000\n",
    "    firstRun = True\n",
    "    possible_classes = dictionary.keys()\n",
    "    for current_class in possible_classes:\n",
    "        if current_class == \"total_data\":\n",
    "            continue\n",
    "        current_class_probability = probability(x, dictionary, current_class)\n",
    "        if(firstRun == True or current_class_probability > best_prob):\n",
    "            best_class = current_class\n",
    "            best_prob = current_class_probability\n",
    "        firstRun = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test,dictionary):\n",
    "    Y_pred = []\n",
    "    num = 0\n",
    "    for x in X_test:\n",
    "        Y_pred.append(predictSingleClass(x, dictionary))\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "print(classification_report(y_pred,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
